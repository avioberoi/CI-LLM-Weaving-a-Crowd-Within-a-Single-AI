model_name: "google/gemma-2-2b"
num_agents: 4

# LoRA Configuration 
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.0   
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

dropconnect_p: 0.0  


trained_peft_checkpoint_dir: "ci_llm_gemma2_2b_K4/checkpoint_step_600"

dataset_name: "gsm8k"
dataset_subset: "main"
max_seq_length_for_prompt: 400   

max_new_tokens: 384  

dirichlet_alpha_aggregator: 1.0

output_dir: "eval_results"
output_file: "gsm8k_results.jsonl"

eval_batch_size: 1   
debug_mode: true   
debug_subset_size: 1  

mixed_precision: "bf16"  