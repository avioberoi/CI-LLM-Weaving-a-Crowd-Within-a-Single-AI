model_name: "google/gemma-2-2b"   
num_agents: 4                        
use_gradient_checkpointing: True     

# LoRA Configuration  
lora_rank: 16                     
lora_alpha: 32                   
lora_dropout: 0.05               
lora_target_modules:              
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
initialize_adapters_rand: True    


dropconnect_p: 0.0               

lambda_sw: 0.01                   
swd_num_projections: 64           

# Training Configuration 
epochs: 3                        
batch_size: 1                     
lr: 3e-5                        
gradient_accumulation_steps: 8    
weight_decay: 0.01                

# LR Scheduler params
lr_scheduler_type: "cosine"      
num_warmup_steps_ratio: 0.03      

# Training limits
max_train_steps: null           

dataset_name: "gsm8k"
dataset_subset: "main"
max_seq_length: 768               
debug_subset_size: null           

output_dir: "ci_llm_gemma3_4b_gsm8k_K4"  
save_steps: 200                  
logging_steps: 10                 
resume_from_checkpoint: null      

num_workers: 4                  

mixed_precision: "bf16"          


use_deepspeed: true             
deepspeed_config_file: "ds_config_zero2.json"  

parallel_mode: "parallel"         

# Profiling
profile_steps: 5                  
log_memory_usage: true            


seed: 42   

# Evaluation Configuration 
eval_batch_size: 2
max_new_tokens_eval: 384        
dirichlet_alpha_aggregator: 1.0